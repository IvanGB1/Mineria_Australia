# -*- coding: utf-8 -*-
"""Evaluación_Parcial3_Anexo3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rCDddbpExWiouT5BTUZH-puKwifY1gkC

# Informe Técnico: Fases 5 y 6 del CRISP-DM

En este informe, aplicaremos la quinta y sexta  fase de la metodología CRISP-DM (Cross-Industry Standard Process for Data Mining) a los datos del archivo "weatherAUS.csv". Estas fases son la Preparación de Datos (Data preparation) y la Transformación de los Datos (Data Trasnformations).

# Fase 1: Comprensión del Negocio (Business Understanding)

## Objetivos del Negocio:


1.   Planificación de Siembras y Cosechas: Utilizar pronósticos meteorológicos para planificar el momento óptimo de siembra y cosecha de cultivos.

2.   Gestión del Riego en la Agricultura: Utilizar datos meteorológicos para optimizar el riego de cultivos y minimizar el desperdicio de agua.

## KPIs Relevantes:



1.   Cuantificar los eventos de altas temperaturas  para prevenir sequías y predecir su impacto en la industria turística y agricultura.

2.   Volumen de precipitación en cierto periodo de tiempo y cómo este se distribuye en las localidades para ver la disponibilidad de agua.

Fecha: Fecha de la observación

Ubicacion: Ubicación de la estación meteorológica

MinTemp: Temperatura mínima en grados Celsius

MaxTemp: Temperatura máxima en grados Celsius

Lluvia: Cantidad de lluvia registrada ese día en mm.

Evaporacion: Evaporación (mm) en 24 horas

Sol: Número de horas de sol brillante en el día

DirRafaga: Dirección de la ráfaga de viento más fuerte en 24 horas.

VelRafaga: Velocidad (km/hr) de la ráfaga de viento más fuerte en 24 horas.

Dir9am: Dirección del viento a las 9am

Dir3pm: Dirección del viento a las 3pm

Vel9am: Velocidad (km/hr) del viento a las 9am

Vel3pm: Velocidad (km/hr) del viento a las 3pm

Hum9am: Porcentaje de humedad a las 9am

Hum3pm: Porcentaje de humedad a las 3pm

Pres9am: Presión atmosférica (hpa) a nivel del mar a las 9am

Pre3pm: Presión atmosférica (hpa) a nivel del mar a las 3pm

Nub9am: Fracción del cielo cubierto por nubes a las 9am. Se mide en "octavos", de
manera que un valor 0 indica cielo totalmente despejado y 8, cielo totalmente
cubierto.

Nub3pm: Fracción del cielo cubierto por nubes a las 3pm. Se mide en "octavos", de
manera que un valor 0 indica cielo totalmente despejado y 8, cielo totalmente
cubierto.

Temp9am: Temperatura en grados Celsius a las 9am

Temp3pm: Temperatura en grados Celsius a las 3pm

LluviaHoy:
Variable indicadora que toma el valor 1 si la precipitación es en mm. en las
últimas 24 hrs. excede 1 mm. y 0 si no.

RISK_MM: La cantidad de lluvia. Una especie de medida del "riesgo".

LluviaMan: Variable indicadora que toma el valor 1 si al día siguiente llovió y
0 si no
"""

### Importar librerias

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import scipy.stats
import seaborn as sns
import sklearn.metrics
import statsmodels.api as sm
import statsmodels.formula.api as smf
import statsmodels.stats as ss
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from scipy.stats import pearsonr
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error

# Para Support Vector Machine (SVM)
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.svm import SVC
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.impute import SimpleImputer

# Para KMEANS
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

##
#digits = datasets.load_digits()

# Importar dataset / .csv
from google.colab import files
uploaded = files.upload()
for fn in uploaded.keys():
    name=fn
df = pd.read_csv(name, sep=",")

"""Características Clave de los Datos:

*   El conjunto de datos contiene observaciones meteorológicas diarias de múltiples ubicaciones en Australia.
*   Variables como temperatura, humedad, velocidad del viento, presión atmosférica y cantidad de lluvia son registradas.
*   La variable objetivo es "Lluvia Man" que indica si llovió al día siguiente.
*   Otras variables como "RISK_MM" proporcionan información adicional sobre la cantidad de lluvia registrada.

Los datos clave para los objetivos de negocio y kpi relevante

1.   Fecha: Esta columna proporciona información sobre la fecha de la observación meteorológica. Es crucial para planificar las siembras y cosechas en función de las condiciones climáticas actuales y futuras.

1.   MinTemp y MaxTemp: Las temperaturas mínimas y máximas son importantes para determinar los momentos óptimos de siembra y cosecha para diferentes cultivos, ya que diferentes cultivos tienen diferentes requisitos de temperatura.

1.   Lluvia: La cantidad de lluvia registrada es esencial para planificar el riego y evitar el exceso de humedad en el suelo, lo que puede afectar negativamente el crecimiento de los cultivos y aumentar el riesgo de enfermedades.

1.   Humedad: Los niveles de humedad en el suelo y en el aire son importantes para determinar las necesidades de riego de los cultivos. La humedad del suelo influye en la absorción de agua por parte de las plantas, mientras que la humedad del aire puede afectar la evaporación del agua del suelo.

1.   Velocidad del Viento: El viento puede influir en la evaporación del agua del suelo y en la distribución del riego. Además, velocidades de viento extremas pueden afectar la calidad de la siembra y la cosecha.

1.   Presión Atmosférica: La presión atmosférica puede proporcionar información sobre los cambios en el clima y las condiciones meteorológicas cambiantes, lo que puede ser útil para prever condiciones adversas que podrían afectar la siembra y cosecha.

1.   Nubosidad: La cantidad de nubes puede afectar la cantidad de luz solar que llega a los cultivos, lo que puede influir en su crecimiento y desarrollo. Además, la nubosidad puede estar relacionada con la probabilidad de lluvia, lo que también es importante para la planificación del riego.

# Fase 2: Comprensión de los datos (Data Understanding)

## Exploración inicial
"""

df

df.columns

df.describe()

"""Se puede identificar que en la informacion de el  Dataset de Australia se encuentran 142193 filas repartidas en 24 columnas, existen columnas importantes como Fecha, las temperaturas mínimas y máximas,

La cantidad de lluvia registrada  

 Los niveles de humedad en el suelo y en el aire son importantes para determinar las necesidades de riego de los cultivos. La humedad del suelo influye en la absorción de agua por parte de las plantas, mientras que la humedad del aire puede afectar la evaporación del agua del suelo.

 El viento puede influir en la evaporación del agua del suelo y en la distribución del riego. Además, velocidades de viento extremas pueden afectar la calidad de la siembra y la cosecha.

 La presión atmosférica puede proporcionar información sobre los cambios en el clima y las condiciones meteorológicas cambiantes, lo que puede ser útil para prever condiciones adversas que podrían afectar la siembra y cosecha.

 La cantidad de nubes puede afectar la cantidad de luz solar que llega a los cultivos, lo que puede influir en su crecimiento y desarrollo. Además, la nubosidad puede estar relacionada con la probabilidad de lluvia, lo que también es importante para la planificación del riego.

## Resumen estadistico basico
"""

#Datos_df es la variable que se utilizara
Datos_df = df

Datos_df.columns

### Cambiamos el nombre de la columna de inglés a español
Datos_df.rename(columns={'Location': 'Ubicacion'}, inplace=True)
Datos_df.rename(columns={'Date': 'Fecha'}, inplace=True)
Datos_df.rename(columns={'Rainfall': 'Lluvia'}, inplace=True)
# columna temperatura minima y maxima matienen su nombre
Datos_df.rename(columns={'Humidity9am': 'Hum9am'}, inplace=True)
Datos_df.rename(columns={'Humidity3pm': 'Hum3pm'}, inplace=True)
Datos_df.rename(columns={'WindGustSpeed': 'VelRafaga'}, inplace=True)
Datos_df.rename(columns={'WindSpeed9am': 'Vel9am'}, inplace=True)
Datos_df.rename(columns={'WindSpeed3pm': 'Vel3pm'}, inplace=True)
Datos_df.rename(columns={'Pressure9am': 'Pres9am'}, inplace=True)
Datos_df.rename(columns={'Pressure3pm': 'Pres3pm'}, inplace=True)
Datos_df.rename(columns={'Cloud9am': 'Nub9am'}, inplace=True)
Datos_df.rename(columns={'Cloud3pm': 'Nub3pm'}, inplace=True)

# Contar la cantidad de registros por ubicación y ordenarlos por frecuencia descendente
Contar = Datos_df['Ubicacion'].value_counts().sort_values(ascending=False)

# Gráfico de barras
plt.figure(figsize=(20,10))
sns.barplot(x=Contar.index, y=Contar.values)
plt.xlabel('Ubicación', fontsize=20)
plt.xticks(rotation=70, fontsize=12)
plt.ylabel('Cantidad de registros', fontsize=14)
plt.title('Distribución de Registros por Ubicación', fontsize=18, fontweight='bold')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()

# Añadir etiquetas en las barras
for i, v in enumerate(Contar.values):
    plt.text(i, v + 10, str(v), color='black', ha='center', fontsize=10)

plt.show()

"""Con este grafico podemos observar que la localidad de Canberra y Sydney son las que poseen mas registros, mientras que Nhil, Katherine y Uluru son las localidades con menor cantidad de registros."""

# Configurar el formato de visualización para mostrar números completos
pd.set_option('display.float_format', lambda x: '%.3f' % x)

Datos_df.describe()

# Calcular la frecuencia de MinTemp
frecuenciaMinTemp = Datos_df['MinTemp'].value_counts()

# Gráfico de barras
plt.bar(frecuenciaMinTemp.index, frecuenciaMinTemp.values, color='#4C72B0')

# Etiquetas al gráfico
plt.title('Frecuencia de temperatura minima en Australia')
plt.xlabel('Temperatura')
plt.ylabel('Frecuencia')
plt.xticks(rotation=90)

plt.show()

"""Podemos observar que la temperatura minima con mayor frecuencia se situa alrededor de 10°C"""

# Calcular la frecuencia de MaxTemp
frecuenciaMinTemp = Datos_df['MaxTemp'].value_counts()

# Crear un gráfico de barras
plt.bar(frecuenciaMinTemp.index, frecuenciaMinTemp.values, color='#4C72B0')

# Agregar etiquetas al gráfico
plt.title('Frecuencia de temperatura maxima en Australia')
plt.xlabel('Temperatura')
plt.ylabel('Frecuencia')
plt.xticks(rotation=90)

# Mostrar el gráfico
plt.show()

"""Podemos observar que la temperatura maxima con mayor frecuencia se situa alrededor de 20°C

## Boxplot e Histograma
"""

# Ajustar la escala del eje Y para mostrar números completos
plt.ticklabel_format(style='plain', axis='y')

### Boxplot con matplotlib
#plt.boxplot(Datos_df['MaxTemp'])
#plt.title('Boxplot de Temperatura Maxima')
#plt.show()

# Histograma con matplotlib
plt.hist(Datos_df['MaxTemp'], bins=100)  # Puedes ajustar el número de bins según tus preferencias
plt.title('Histograma de Temperatura Maxima')
plt.show()

sns.histplot(Datos_df.Lluvia)
plt.xlim(0, 1);

print(Datos_df.columns)

"""## Valores estadisticos"""

#Resumen estadistico
columna = ['MinTemp', 'MaxTemp', 'Lluvia', 'Evaporation', 'Sunshine', 'VelRafaga',
            'Vel9am', 'Vel3pm', 'Pres9am', 'Temp9am', 'Temp3pm', 'RISK_MM']



mediana = Datos_df[columna].median().round(2)
media = Datos_df[columna].mean().round(2)

# Calcular la moda para las columnas seleccionadas
moda = Datos_df[columna].mode().iloc[0].round(2)

# Calcular la varianza para las columnas seleccionadas
varianza = Datos_df[columna].var().round(2)

# Calcular el mínimo para las columnas seleccionadas
minimo = Datos_df[columna].min().round(2)

# Calcular el máximo para las columnas seleccionadas
maximo = Datos_df[columna].max().round(2)

# Calcular la desviación estándar para las columnas seleccionadas
desviacion_estandar = Datos_df[columna].std().round(2)

# Calcular el coeficiente de variación para las columnas seleccionadas
coeficiente_variacion = ((desviacion_estandar / media) * 100).round(2)
resultado_final = ""

# Mostrar los resultados para cada columna en la misma línea
for col in columna:

    print(f"{col}: Mediana={mediana[col]}, Media={media[col]}, Moda={moda[col]}, Varianza={varianza[col]}, Mínimo={minimo[col]}, Máximo={maximo[col]}, Desv. Estándar={desviacion_estandar[col]}, Coef. de Variación={coeficiente_variacion[col]}")
    print("")

# Seleccionar las columnas para el cálculo de la matriz de correlación
columnas = ['MinTemp', 'MaxTemp', 'Lluvia', 'Evaporation', 'Sunshine', 'VelRafaga',
            'Vel9am', 'Vel3pm', 'Pres9am', 'Temp9am', 'Temp3pm', 'RISK_MM']

# Crear un DataFrame con las columnas seleccionadas
df_seleccionado = Datos_df[columnas]

# Calcular la matriz de correlación
matriz_correlacion = df_seleccionado.corr()

# Mostrar la matriz de correlación
print("Matriz de correlación:")
print(matriz_correlacion)

"""## 3. Matriz de Correlacion | 4. Diagrama de dispersion"""

# Seleccionar las columnas para el cálculo de la matriz de correlación
columnas = ['MinTemp', 'MaxTemp', 'Lluvia', 'Evaporation', 'Sunshine',
                        'VelRafaga', 'Vel9am', 'Vel3pm', 'Pres9am', 'Pres3pm',
                        'Temp9am', 'Temp3pm', 'Hum9am', 'Hum3pm', 'Nub9am', 'Nub3pm']

# Crear un DataFrame con las columnas seleccionadas
df_seleccionado = Datos_df[columnas]

# Calcular la matriz de correlación
matriz_correlacion = df_seleccionado.corr()

# Mostrar la matriz de correlación
print("Matriz de correlación:")
print(matriz_correlacion)

# Paso 1: Seleccionar las variables climáticas relevantes
matriz_correlacion = df_seleccionado.corr()

# Visualizar la matriz de correlación utilizando seaborn
plt.figure(figsize=(20, 15))
sns.heatmap(matriz_correlacion, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Matriz de Correlación')
plt.show()

"""Relaciones más utiles

En lo relacionado con meteorologia, cuando la temperatura es alta, hay mayor humedad ambiental, por lo cual habra mayor cantidad de vapor de agua en el aire y la presion atmosferica aumenta. Cuando sucede esto, hay mayor probabilidad de precipitaciones en el día.Segun esto, hemos decidido elegir las siguientes relaciones.

Estas variables tienen una relación positiva, por lo cual si una aumenta, la otra también, ademas de que la utilicemos más adelante.


*   Luz solar(Sunshine) con temperaturas máximas(MaxTemp) con un 33% de relacion


*   Luz solar(Sunshine) con Evaporacion(Evaporation) con un 29% de relacion


*   Evaporacion(Evaporation) con Máxima temperatura(MaxTemp)un 44% de relacion

*   Evaporacion(Evaporation) con Minima temperatura(MinTemp)un 35% de relacion


*   Evaporacion(Evaporation) con Minima temperatura(MinTemp)un 35% de relacion



*   Evaporacion(Evaporation) con Temperatura a las 3pm(Temp3pm)un 43% de relacion



*   Evaporacion(Evaporation) con Temperatura a las 3pm(Temp3pm)un 43% de relacion


*   Nuvosidad(Num3pm) con Humedad a las 3pm(Hum3pm)un 41% de relacion

*   Lluvia(Lluvia) con Humedad a las 3pm(Hum3pm)un 25% de relacion

Estas variables son negativas, por lo que si una aumenta, la otra disminuye




*   Luz solar(Sunshine) con Nubosidad a las 9am y 3pm(Num9am-Num3pm) con un -54% de relación

*   Humedad(9am y 3pm) con Temperatura a las 3pm(Temp3pm)un -47% de relación

*   Luz solar(Sunshine) con Humedad las 9am y 3pm(Hum9am-Hum3pm) con un -45% de relación




"""

# Variables climáticas
variables_climaticas = ['MinTemp', 'MaxTemp', 'Lluvia', 'Evaporation', 'Sunshine',
                        'VelRafaga', 'Vel9am', 'Vel3pm', 'Pres9am', 'Pres3pm',
                        'Temp9am', 'Temp3pm', 'Hum9am', 'Hum3pm', 'Nub9am', 'Nub3pm']

# Gráficos de dispersión
for variable in variables_climaticas:
    plt.figure(figsize=(8, 6))
    plt.scatter(Datos_df.index, Datos_df[variable], alpha=0.7)
    plt.title(f'Gráfico de Dispersión: {variable}', fontsize=14)
    plt.xlabel('Índice de Datos', fontsize=12)
    plt.ylabel('Valor', fontsize=12)
    plt.grid(True)
    plt.tight_layout()
    plt.show()

"""Se encuentra presencia de outliers en algunas variables, esto puede dificultar al realizar análisis posteriores o modelos predictivos. Por lo cual se debe eliminar o corregirlos en una posterior etapa.


"""

Datos_df.columns

# Paso 1: Análisis de variables meteorológicas
variables_climaticas = ['MinTemp', 'MaxTemp', 'Lluvia', 'Evaporation', 'Sunshine', 'VelRafaga', 'Vel9am', 'Vel3pm', 'Hum9am', 'Hum3pm', 'Pres9am', 'Pres3pm', 'Nub9am', 'Nub3pm', 'Temp9am', 'Temp3pm']

# Paso 2: Identificación de umbrales anómalos (por ejemplo, utilizando el rango intercuartílico)
Q1 = Datos_df[variables_climaticas].quantile(0.25)
Q3 = Datos_df[variables_climaticas].quantile(0.75)
IQR = Q3 - Q1
umbral_inferior = Q1 - 1.5 * IQR
umbral_superior = Q3 + 1.5 * IQR

# Paso 3: Visualización de datos
plt.figure(figsize=(15, 10))
for i, variable in enumerate(variables_climaticas, 1):
    plt.subplot(4, 4, i)
    sns.boxplot(x=df[variable])
    plt.title(variable)
plt.tight_layout()
plt.show()

"""De estas graficas de boxplot podemos observar:

MinTemp su promedio se encuentra en unos 12°C y se distribuye en un rango entre 8°C y 17°C aproximadamente

Las horas de sol tienen un rango de 6 a 10 horas

La lluvia tiene valores atipicos superior a 300mm.

Evaporacion tiene valor atipico que supera 140mm.

Velocidad de rafaga tiene valores atipicos que superan 120km/h

La humedad a las 9am es en promedio de 70% y en un rango de 58% a 86%

La nubosidad de 9am y 3pm se encuentran en un rango de 3/8 a 6/8
"""

# Paso 4:  comparación con datos históricos para identificar si las condiciones climáticas actuales son inusuales o anormales en un período específico del año


import math

# Paso 1: Convertir la columna de fechas a tipo datetime
Datos_df['Fecha'] = pd.to_datetime(Datos_df['Fecha'])

# Paso 2: Filtrar el conjunto de datos para obtener solo los datos históricos (por ejemplo, hasta 2017-05-29)
datos_historicos = Datos_df[Datos_df['Fecha'] < '2017-05-30']

# Paso 3: Calcular estadísticas históricas
estadisticas_historicas = datos_historicos[variables_climaticas].describe()

# Paso 4: Comparar con datos actuales
anomalias = Datos_df[variables_climaticas].apply(lambda x: (x - estadisticas_historicas.loc['mean']) / estadisticas_historicas.loc['std'])


# Paso 5: Comparación con datos históricos

# Calcular el número necesario de filas y columnas en la figura
num_variables = len(variables_climaticas)
num_rows = math.ceil(num_variables / 3)
num_cols = min(num_variables, 3)

# Crear la figura con el tamaño adecuado
plt.figure(figsize=(num_cols * 5, num_rows * 4))

# Iterar sobre las variables climáticas y graficar las anomalías
for i, variable in enumerate(variables_climaticas, 1):
    plt.subplot(num_rows, num_cols, i)
    sns.histplot(anomalias[variable], kde=True, color='blue', bins=20)
    plt.axvline(0, color='red', linestyle='--')
    plt.title(f'Anomalía en {variable}')
    plt.xlabel('Valor Normalizado')
    plt.ylabel('Frecuencia')

plt.tight_layout()
plt.show()

"""# Fase 3: Preparación de los datos

## Limpieza de datos
"""

### Revisamos que columnas tienen valores nulos para limpiar
(
    df
    .isnull()
    .any()
)

"""Al revisar la columnas para identificar si existen valores nulos, se detectó en las siguientes variables:
MinTemp           
MaxTemp           
Rainfall          
Evaporation       
Sunshine          
WindGustDir       
WindGustSpeed     
WindDir9am        
WindDir3pm        
WindSpeed9am      
WindSpeed3pm      
Humidity9am       
Humidity3pm       
Pressure9am       
Pressure3pm       
Cloud9am          
Cloud3pm          
Temp9am           
Temp3pm           
RainToday
"""

#Para saber la cantidad de nulos en cada columna
df.isna().sum()

"""Al identificar la cantidad de valores nulos, se puede observar que es una gran cantidad de ellos.

"""

### Revisamos cuantos valores nulos tenemos
(
    df
    .isnull()
    .sum()
    .sum()
)

"""La cantidad total de nulos es de 316559

"""

#Para saber que tipo de datos es cada columna
df.dtypes

# crea un dataframe para relizar limpieza de datos nulos

limpieza_df = pd.read_csv('weatherAUS.csv')
df = limpieza_df

#Revisamos la agrupación de los valores nulo en las columnas
(
    limpieza_df
    .isnull()
    .melt()
    .pipe(
        lambda df: (
            sns.displot(
                data=df,
                y='variable',
                hue='value',
                multiple='fill',
                aspect=2
            )
        )
    )
)

"""En el siguiente gráfico se puede observar la cantidad de valores nulos, en los cuales las columnas de Sunshine, Evaporation, Cloud9am, Cloud3pm presentan la mayor cantidad."""

#CANTIDAD DE DATOS AGRUPADOS EN CADA COLUMNA. El mapa de calor muestra la distribución de valores nulos en limpieza_df, donde las filas representan las columnas del DataFrame original y las columnas representan las filas originales.
(
    limpieza_df
    .isnull()
    .transpose()
    .pipe(
        lambda df: (
            sns.heatmap(
                data= df
            )
        )
    )
)

#Cantidad de datos eliminados si se procesa un eliminado de datos faltantes
limpieza_df.shape

#copiado a una nueva variable
limpieza_df2=limpieza_df.copy()

#.   el primer valor representa el número de filas y la segunda entrada representa el número de columnas en el DataFrame limpieza_df2
limpieza_df2.shape

limpieza_total = (
    limpieza_df2
    .dropna()
)
limpieza_total

"""Se puede observar que se han eliminado un 60% de los datos, lo cual no es aceptable"""

limpieza_df.dtypes

limpieza_df.columns

#Cambio de tip de dato de fecha de object a datetime
Datos_df['Fecha'] = pd.to_datetime(Datos_df['Fecha'])

#Se agruparan segun el lugar y se calculara su media para llenar los datos faltantes
Agrupacion = limpieza_df.groupby('Location')[['MinTemp', 'MaxTemp','Evaporation', 'Sunshine','WindGustSpeed',
                                              'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm','Cloud9am',
                                              'Cloud3pm','Temp9am','Temp3pm','RISK_MM']].mean()

# Calcular la media global de cada columna

Media = limpieza_df[['MinTemp', 'MaxTemp','Evaporation', 'Sunshine','WindGustSpeed',
                     'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm',
                     'Pressure9am', 'Pressure3pm','Cloud9am','Cloud3pm','Temp9am',
                     'Temp3pm','RISK_MM']].mean()

for columna in Agrupacion.columns:
    limpieza_df[columna].fillna(value=Media[columna], inplace=True)

#Se ingresara a los valores faltantes con un indefinido
limpieza_df['WindGustDir'] = limpieza_df['WindGustDir'].fillna('Indefinido')
limpieza_df['WindDir9am'] = limpieza_df['WindDir9am'].fillna('Indefinido')
limpieza_df['WindDir3pm'] = limpieza_df['WindDir3pm'].fillna('Indefinido')
#Se le aignara el valor 0 a los nulos de Lluvia al ser un lugar mayormente de temperaturas altas y lluvia con un No
limpieza_df['Rainfall'] = limpieza_df['Rainfall'].fillna(0)
limpieza_df['RainToday'] = limpieza_df['RainToday'].fillna('No')

limpieza_df.isna().sum()

"""Se limpiaron los valores nulos agrupando segun el lugar y rellenando segun la media para no perder un volumen grande de informacion.

# Fase 4: Modelado de datos

## Selección de Modelos:

*   Modelos Supervisados:

1.   Regresión lineal multiple: Para la predicción de variables continuas usando múltiples características.
2.   Regresión lineal: Para la clasificación binaria.
3.   Máquina de soporte vectorial (SVM): Para la clasificación binaria.
4.   Bosque aleatorio (Random Forest): Para la clasificación binaria.

*   Modelos No Supervisados:

1.   K-Means: Para la agrupación de datos.
2.   Clustering jerárquico: Para la agrupación de datos.

En esta fase para cada modelo se realizara una seleccion de datos a utilizar, una division de datos para entrenar y testear, hacer el modelo y posteriormente una evaluacion.

## Modelado Supervisado:

### 1.   Regresión lineal multiple:
"""

RLM = limpieza_df.copy()

RLM.isna().sum()

#Las variables elegidas son 'MinTemp', 'MaxTemp', 'Evaporation', 'Sunshine', 'Pressure9am', 'Pressure3pm', 'Temp9am', 'Temp3pm', 'RainToday' para predecir Rainfall.

RLM['RainToday'] = RLM['RainToday'].map({'No': 0, 'Yes': 1})


X = RLM[['MinTemp', 'MaxTemp', 'Evaporation', 'Sunshine', 'Pressure9am', 'Pressure3pm', 'Temp9am', 'Temp3pm', 'RainToday']] # Variables independientes
y = RLM['Rainfall'] # Variable dependiente

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Crear el modelo de regresión
modelo = LinearRegression()

# Entrenar el modelo
modelo.fit(X_train, y_train)

# Información del modelo

# Realizar predicciones
y_pred = modelo.predict(X_test)

# Calcular el error cuadrático medio (MSE)
mse = mean_squared_error(y_test, y_pred)

# Calcular el coeficiente de determinación (R^2)
r2 = r2_score(y_test, y_pred)

print(f'Error cuadrático medio (MSE): {mse}')
print(f'Coeficiente de determinación (R^2): {r2}')

"""Los resultados de este modelo son:

Error cuadrático medio(MSE): 53.64
El MSE de 53.64 indica que el modelo es poco preciso en los valores reales.

Coeficiente de determinación: 0.26
El coeficiente de determinación de 0.26  muestra lo mal que se ajusta un modelo de regresión a un conjunto de datos.

"""

# Grafico para visalización de datos
variables_predictoras = ['MinTemp', 'MaxTemp', 'Evaporation', 'Sunshine', 'Pressure9am', 'Pressure3pm', 'Temp9am', 'Temp3pm', 'RainToday']

plt.figure(figsize=(20, 20))
for i, var in enumerate(variables_predictoras):
    plt.subplot(4, 3, i + 1)
    sns.scatterplot(x=X_test[var], y=y_test, alpha=0.5)
    sns.lineplot(x=X_test[var], y=modelo.predict(X_test), color='red')
    plt.xlabel(var)
    plt.ylabel('RISK_MM')
    plt.title(f'Relación entre {var} y Lluvia')
plt.tight_layout()
plt.show()

"""Los gráficos indican que no hay una fuerte correlación lineal entre la mayoría de las variables meteorológicas individuales y la cantidad de lluvia en Australia. La única relación clara es que si llovió ese día, la cantidad de lluvia es mayor que cero.

### 2.   Regresión lineal:
"""

RL = limpieza_df.copy()

RL.isna().sum()

#Las variables elegidas son Humidity9am para predecir Cloud9am. La nubosidad influye en la posibilidad de lluvia.

# Variable independientes
X = RL[['Humidity9am']]
# Variable dependiente
y = RL['Rainfall']


# Dividir los datos en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Crear y entrenar el modelo de regresión lineal
modelo = LinearRegression()
modelo.fit(X_train, y_train)

# Información del modelo

# Realizar predicciones
y_pred = modelo.predict(X_test)

# Calcular el error cuadrático medio (MSE)
mse = mean_squared_error(y_test, y_pred)

# Calcular el coeficiente de determinación (R^2)
r2 = r2_score(y_test, y_pred)

print(f'Error cuadrático medio (MSE): {mse}')
print(f'Coeficiente de determinación (R^2): {r2}')

"""Los resultados de este modelo son:

Error cuadrático medio(MSE): 69.45
El MSE de  69.45 indica que el modelo es de precisión media en los valores reales.

Coeficiente de determinación: 0.04
El coeficiente de determinación de 0.04  muestra lo mal que se ajusta un modelo de regresión a un conjunto de datos.

"""

# Grafico para viausalización de datos
plt.figure(figsize=(10, 6))
plt.scatter(X_test, y_test, color='black', label='Datos reales')
plt.plot(X_test, y_pred, color='red', linewidth=2, label='Modelo de regresión')
plt.xlabel('Humedad')
plt.ylabel('Nubosidad')
plt.title('Regresión Lineal')
plt.legend()
plt.show()

"""La línea de regresión es casi plana, lo que indica una relación muy débil o inexistente.

Los puntos están muy dispersos alrededor de la línea de regresión, lo que refuerza la idea de que la relación entre las variables es débil.

El modelo no parece ser muy preciso para predecir la nubosidad a partir de la humedad, debido a la falta de una relación clara entre ambas variables.

### 3.   Bosques aleatorios (Random Forest):
"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

RM = limpieza_df.copy()
RM_nuevo = limpieza_df.copy()

RM['RainToday'] = RM['RainToday'].map({'No': 0, 'Yes': 1})
sns.scatterplot(
    data = RM,
    x = "Humidity9am",
    y = "Rainfall",
    hue = "RainToday",
    alpha = 0.5
)

"""Existe una relación positiva entre la humedad a las 9 AM y la probabilidad de lluvia en Australia.

En los días con lluvia, la humedad tiende a ser más alta que en los días sin lluvia.

La cantidad de lluvia en un día con lluvia tiende a aumentar con la humedad, aunque esta relación no es perfectamente lineal.
"""

def entropy(data):
    classes = np.unique(data)
    entropies = []
    for c in classes:
        p = sum(data == c) / len(data)
        current_entropy = p * np.log2(p)
        entropies.append(current_entropy)
    return -1 * sum(entropies)

entropy(RM.RainToday)

entropy(RM.RainToday[RM.Humidity9am >= 52])

"""### 4.   Maquina de soporte vectorial (SVM)

**¿Cómo se puede predecir la lluvia utilizando los datos meteorológicos, con el fin de optimizar la planificación agrícola y reducir los riesgos asociados a las condiciones climáticas adversas?**

El modelo SVM entrenado con las características seleccionadas sera capaz de predecir si lloverá mañana o no. Esto podría ayudar a abordar la problemática de la incertidumbre en la planificación agrícola y en la toma de decisiones relacionadas con el riego de cultivos.
"""

# Copia el DataFrame original para evitar alterarlo
df_seleccion = limpieza_df.copy()

# Convierte la variable objetivo 'RainTomorrow' a valores binarios en la copia del DataFrame
label_encoder = LabelEncoder()
df_seleccion['RainTomorrow'] = label_encoder.fit_transform(df_seleccion['RainTomorrow'])

# Selecciona las características a utilizar
caracteristicas = ['MinTemp', 'MaxTemp', 'Rainfall', 'Humidity9am', 'Humidity3pm', 'RainTomorrow']

# Crea un nuevo DataFrame con las características seleccionadas
df_seleccion = df_seleccion[caracteristicas]

# Dividide los datos en características (X) y variable objetivo (y)
X = df_seleccion.drop(columns=['RainTomorrow'])
y = df_seleccion['RainTomorrow']

# Divide los datos en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Crear un pipeline que primero preprocesa los datos y luego ajusta el modelo SVM
pipeline = Pipeline(steps=[
    ('scaler', StandardScaler()),  # Escalaa las características para mejorar el rendimiento del modelo
    ('svm', SVC(kernel='rbf', random_state=42))
])

# Ajusta el modelo SVM
pipeline.fit(X_train, y_train)

#pd: la ejecucion me tarda aprox 10 minutos, pero compila sin errores.

# Evaluar el modelo
accuracy = pipeline.score(X_test, y_test)
print("Precisión del modelo SVM:", accuracy)

"""La precisión del modelo SVM con los datos seleccionados es 83% aprox.

## Modelos no supervisados:

### 1.   K-Means:

Este algoritmo KMEANS se enfoca en la segmentación de las ubicaciones en función de patrones climáticos similares. Esto podría ayudar en la planificación de siembras y cosechas, así como en la gestión del riego, al identificar regiones con condiciones climáticas comparables.
"""

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score

# Selecciona las características importantes para la segmentación
caracteristicas = ['MinTemp', 'MaxTemp', 'Rainfall', 'Humidity9am', 'Humidity3pm']

# Crea un nuevo DataFrame con las características seleccionadas
df_seleccion = limpieza_df[caracteristicas].copy()

# Normaliza los datos para que todas las características estén en la misma escala
scaler = StandardScaler()
df_seleccion_scaled = scaler.fit_transform(df_seleccion)

# Determina el número óptimo de clústeres utilizando el método del codo
inertia = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(df_seleccion_scaled)
    inertia.append(kmeans.inertia_)

# Grafica el método del codo
plt.plot(range(1, 11), inertia, marker='o')
plt.xlabel('Número de clústeres')
plt.ylabel('Inertia')
plt.title('Método del codo para determinar el número óptimo de clústeres')
plt.show()

# Basado en el método del codo, selecciona un número óptimo de clústeres
k_optimo = 3

# Aplicar K-means con el número óptimo de clústeres
kmeans = KMeans(n_clusters=k_optimo, random_state=42)
kmeans.fit(df_seleccion_scaled)

# Añade la columna 'Cluster' al DataFrame original
limpieza_df['Cluster'] = kmeans.labels_

# Visualizar los clústeres en un diagrama de dispersión (por ejemplo, para las características MinTemp y MaxTemp)
plt.scatter(limpieza_df['MinTemp'], limpieza_df['MaxTemp'], c=limpieza_df['Cluster'], cmap='viridis')
plt.xlabel('Temperatura Mínima')
plt.ylabel('Temperatura Máxima')
plt.title('Segmentación de ubicaciones basada en características climáticas')
plt.colorbar(label='Clúster')
plt.show()

"""*   Clúster 0 (amarillo): Representa ubicaciones con
temperaturas mínimas y máximas más bajas. Estas podrían ser zonas montañosas o regiones del sur de Australia con climas más fríos.

*   Clúster 1 (morado): Representa ubicaciones con temperaturas mínimas moderadas y temperaturas máximas más altas. Estas podrían ser zonas del interior de Australia con mayor variación de temperatura entre el día y la noche.

*   Clúster 2 (verde): Representa ubicaciones con temperaturas mínimas y máximas más altas. Estas podrían ser zonas costeras o regiones del norte de Australia con climas más cálidos.

La dispersión de los puntos dentro de cada clúster indica que, aunque las ubicaciones comparten características climáticas similares, también hay variaciones individuales en las temperaturas.



"""

# Evaluación del modelo
# Coeficiente de Silueta
silhouette_avg = silhouette_score(df_seleccion_scaled, limpieza_df['Cluster'])

# Índice de Davies-Bouldin
db_index = davies_bouldin_score(df_seleccion_scaled, limpieza_df['Cluster'])

print(f'Coeficiente de Silueta: {silhouette_avg}')
print(f'Índice de Davies-Bouldin: {db_index}')

"""### 2.   Clustering Jerárquico:

¿Existen patrones climáticos regionales en Australia que influyen en la ocurrencia de lluvias significativas y cómo se pueden segmentar estos patrones para mejorar la planificación agrícola y la gestión de recursos hídricos?
"""

# Función para preparar los datos
def prepare_data(df, sample_size=None):
    # Convertir las columnas 'RainToday' y 'RainTomorrow' a binarias
    if 'RainToday' in df.columns:
        df['RainToday'] = df['RainToday'].map({'No': 0, 'Yes': 1})
    if 'RainTomorrow' in df.columns:
        df['RainTomorrow'] = df['RainTomorrow'].map({'No': 0, 'Yes': 1})

    # Seleccionar características relevantes
    caracteristicas = ['MinTemp', 'MaxTemp', 'Rainfall', 'Humidity9am', 'Humidity3pm', 'Temp9am', 'Temp3pm', 'RainTomorrow']
    df_seleccion = df[caracteristicas].dropna()

    if sample_size is not None:
        df_seleccion = df_seleccion.sample(n=sample_size, random_state=42)

    # Normalizar los datos
    scaler = StandardScaler()
    df_scaled = scaler.fit_transform(df_seleccion)

    return df_seleccion, df_scaled

# Utilizar una muestra del dataset (p.ej., 5000 filas) para manejar problemas de memoria
df_seleccion, df_scaled = prepare_data(df, sample_size=5000)

# Dividir los datos en conjuntos de entrenamiento y prueba
from sklearn.model_selection import train_test_split
X_train, X_test = train_test_split(df_scaled, test_size=0.3, random_state=42)

from scipy.cluster.hierarchy import dendrogram, linkage, fcluster

# Aplicar clustering jerárquico al conjunto de entrenamiento
linked = linkage(X_train, method='ward')

# Crear el dendrograma
plt.figure(figsize=(10, 7))
dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=True)
plt.title('Dendrograma de Clustering Jerárquico (Conjunto de Entrenamiento)')
plt.xlabel('Índice de Muestra')
plt.ylabel('Distancia')
plt.show()

# Determinar el número óptimo de clústeres
num_clusters = 3  # Ajustar basado en el dendrograma

# Obtener etiquetas de clústeres para el conjunto de entrenamiento
etiquetas_train = fcluster(linked, num_clusters, criterion='maxclust')

# Aplicar clustering jerárquico al conjunto de prueba
linked_test = linkage(X_test, method='ward')
etiquetas_test = fcluster(linked_test, num_clusters, criterion='maxclust')

"""*   Dos grupos principales: El dendrograma sugiere que los datos se dividen en dos grupos principales a una distancia considerable. Esto indica una clara diferencia en las características climáticas entre estos dos grupos.
*   Subdivisiones: Cada uno de los grupos principales se subdivide en varios clústeres más pequeños. Esto sugiere que hay variaciones climáticas significativas dentro de cada grupo principal.

"""

from sklearn.metrics import silhouette_score

# Calcular el Silhouette Score para el conjunto de entrenamiento
silhouette_train = silhouette_score(X_train, etiquetas_train)
print(f'Silhouette Score del modelo de clustering jerárquico (Conjunto de Entrenamiento): {silhouette_train}')

# Calcular el Silhouette Score para el conjunto de prueba
silhouette_test = silhouette_score(X_test, etiquetas_test)
print(f'Silhouette Score del modelo de clustering jerárquico (Conjunto de Prueba): {silhouette_test}')

# Añadir etiquetas de clúster al DataFrame original (para análisis)
df_seleccion_train = pd.DataFrame(X_train, columns=['MinTemp', 'MaxTemp', 'Rainfall', 'Humidity9am', 'Humidity3pm', 'Temp9am', 'Temp3pm', 'RainTomorrow'])
df_seleccion_train['Cluster'] = etiquetas_train

df_seleccion_test = pd.DataFrame(X_test, columns=['MinTemp', 'MaxTemp', 'Rainfall', 'Humidity9am', 'Humidity3pm', 'Temp9am', 'Temp3pm', 'RainTomorrow'])
df_seleccion_test['Cluster'] = etiquetas_test

# Analizar patrones climáticos en cada clúster (Conjunto de Entrenamiento)
for cluster in range(1, num_clusters + 1):
    print(f'\nPatrones climáticos en el clúster {cluster} (Conjunto de Entrenamiento):')
    print(df_seleccion_train[df_seleccion_train['Cluster'] == cluster].describe())

# Analizar patrones climáticos en cada clúster (Conjunto de Prueba)
for cluster in range(1, num_clusters + 1):
    print(f'\nPatrones climáticos en el clúster {cluster} (Conjunto de Prueba):')
    print(df_seleccion_test[df_seleccion_test['Cluster'] == cluster].describe())

# Visualizar clústeres en un diagrama de dispersión (por ejemplo, Temp9am y Temp3pm)
plt.scatter(df_seleccion_train['Temp9am'], df_seleccion_train['Temp3pm'], c=df_seleccion_train['Cluster'], cmap='viridis')
plt.xlabel('Temperatura a las 9am')
plt.ylabel('Temperatura a las 3pm')
plt.title('Segmentación de ubicaciones basada en características climáticas (Conjunto de Entrenamiento)')
plt.colorbar(label='Clúster')
plt.show()

plt.scatter(df_seleccion_test['Temp9am'], df_seleccion_test['Temp3pm'], c=df_seleccion_test['Cluster'], cmap='viridis')
plt.xlabel('Temperatura a las 9am')
plt.ylabel('Temperatura a las 3pm')
plt.title('Segmentación de ubicaciones basada en características climáticas (Conjunto de Prueba)')
plt.colorbar(label='Clúster')
plt.show()

"""Conjunto de entrenamiento:

*   Clúster 0 (violeta): Representa ubicaciones con temperaturas relativamente altas tanto a las 9 AM como a las 3 PM. Estas podrían ser zonas del interior o del norte de Australia con climas más cálidos y menos variación de temperatura a lo largo del día.

*   Clúster 1 (amarillo): Representa ubicaciones con temperaturas moderadas a las 9 AM y temperaturas más altas a las 3 PM. Estas podrían ser zonas con una mayor amplitud térmica diaria, donde las mañanas son frescas y las tardes cálidas.

*   Clúster 2 (verde): Representa ubicaciones con temperaturas bajas tanto a las 9 AM como a las 3 PM. Estas podrían ser zonas montañosas o del sur de Australia con climas más fríos.

Conjunto de prueba:

*   Clúster 0 (violeta): Representa ubicaciones con temperaturas relativamente altas tanto a las 9 AM como a las 3 PM. Estas podrían ser zonas del interior o del norte de Australia con climas más cálidos y menos variación de temperatura a lo largo del día.

*   Clúster 1 (amarillo): Representa ubicaciones con temperaturas moderadas a las 9 AM y temperaturas más altas a las 3 PM. Estas podrían ser zonas con una mayor amplitud térmica diaria, donde las mañanas son frescas y las tardes cálidas.

*   Clúster 2 (verde): Representa ubicaciones con temperaturas bajas tanto a las 9 AM como a las 3 PM. Estas podrían ser zonas montañosas o del sur de Australia con climas más fríos.

# Resumen

Tras realizar un análisis, hemos implementado una estrategia para abordar los valores nulos en nuestro conjunto de datos climáticos. Esta estrategia se centra en la utilización de la media o mediana de cada columna, agrupando los datos por ubicación, para reemplazar los valores faltantes. Esto permite conservar la consistencia de los datos, al considerar las características de cada región. Además, para las variables categóricas relacionadas con la precipitación, hemos optado por asignar un valor predeterminado, como "No", basándonos en la climatología de Australia, un país conocido por sus condiciones climáticas cálidas y con menos probabilidad de lluvias constantes en comparación con otras regiones.
En el proceso de selección del modelo con mayor precisión para predecir la lluvia, el modelo de Support Vector Machine (SVM) resultó ser el más preciso, logrando una exactitud de aproximadamente el 83%.

# Fase 5: Evaluacion

## Evaluación de modelos supervisados

### Regresión lineal múltiple:
"""

RLM1 = limpieza_df.copy()

RLM1.isna().sum()

#Las variables elegidas son 'MinTemp', 'MaxTemp', 'Evaporation', 'Sunshine', 'Pressure9am', 'Pressure3pm', 'Temp9am', 'Temp3pm', 'RainToday' para predecir Rainfall.

RLM1['RainToday'] = RLM1['RainToday'].map({'No': 0, 'Yes': 1})


X = RLM1[['MinTemp', 'MaxTemp', 'Evaporation', 'Sunshine', 'Pressure9am', 'Pressure3pm', 'Temp9am', 'Temp3pm', 'RainToday']] # Variables independientes
y = RLM1['Rainfall'] # Variable dependiente

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Crear el modelo de regresión
modelo = LinearRegression()

# Entrenar el modelo
modelo.fit(X_train, y_train)

# Información del modelo

# Realizar predicciones
y_pred = modelo.predict(X_test)

# Calcular el error cuadrático medio (MSE)
mse = mean_squared_error(y_test, y_pred)

# Calcular el coeficiente de determinación (R^2)
r2 = r2_score(y_test, y_pred)

print(f'Error cuadrático medio (MSE): {mse}')
print(f'Coeficiente de determinación (R^2): {r2}')

"""Los resultados de este modelo son:

Error cuadrático medio(MSE): 53.64
El MSE de 53.64 indica que el modelo es poco preciso en los valores reales.

Coeficiente de determinación: 0.26
El coeficiente de determinación de 0.26  muestra lo mal que se ajusta un modelo de regresión a un conjunto de datos.

### Regresión lineal:
"""

RL1 = limpieza_df.copy()

#Las variables elegidas son Humidity9am para predecir Cloud9am. La nubosidad influye en la posibilidad de lluvia.

# Variable independientes
X = RL1[['Humidity9am']]
# Variable dependiente
y = RL1['Rainfall']


# Dividir los datos en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Crear y entrenar el modelo de regresión lineal
modelo = LinearRegression()
modelo.fit(X_train, y_train)

# Información del modelo

# Realizar predicciones
y_pred = modelo.predict(X_test)

# Calcular el error cuadrático medio (MSE)
mse = mean_squared_error(y_test, y_pred)

# Calcular el coeficiente de determinación (R^2)
r2 = r2_score(y_test, y_pred)

print(f'Error cuadrático medio (MSE): {mse}')
print(f'Coeficiente de determinación (R^2): {r2}')

"""Los resultados de este modelo son:

Error cuadrático medio(MSE): 69.45
El MSE de  69.45 indica que el modelo es de precisión media en los valores reales.

Coeficiente de determinación: 0.04
El coeficiente de determinación de 0.04  muestra lo mal que se ajusta un modelo de regresión a un conjunto de datos.

### Bosques aleatorios (Random Forest)
"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

DFO = limpieza_df.copy()

DFO['RainToday'] = DFO['RainToday'].map({'No': 0, 'Yes': 1})
sns.scatterplot(
    data = DFO,
    x = "Humidity9am",
    y = "Rainfall",
    hue = "RainToday",
    alpha = 0.5
)

def entropy(data):
    classes = np.unique(data)
    entropies = []
    for c in classes:
        p = sum(data == c) / len(data)
        current_entropy = p * np.log2(p)
        entropies.append(current_entropy)
    return -1 * sum(entropies)

entropy(DFO.RainToday)

entropy(DFO.RainToday[DFO.Humidity9am >= 52])

"""Los resultados de este modelo son:

Entropía general (0.7624): Indica el nivel de incertidumbre general en el dataset sobre si lloverá hoy.

Entropía con Humedad >= 52 (0.8260): Indica una mayor incertidumbre específica bajo esta condición, sugiriendo que la humedad a las 9 AM mayor o igual a 52 no es un predictor claro de lluvia.

### Máquina de soporte vectorial (SVM)
"""

# Copia el DataFrame original para evitar alterarlo
df_seleccion = limpieza_df.copy()

# Convierte la variable objetivo 'RainTomorrow' a valores binarios en la copia del DataFrame
label_encoder = LabelEncoder()
df_seleccion['RainTomorrow'] = label_encoder.fit_transform(df_seleccion['RainTomorrow'])

# Selecciona las características a utilizar
caracteristicas = ['MinTemp', 'MaxTemp', 'Rainfall', 'Humidity9am', 'Humidity3pm', 'RainTomorrow']

# Crea un nuevo DataFrame con las características seleccionadas
df_seleccion = df_seleccion[caracteristicas]

# Dividide los datos en características (X) y variable objetivo (y)
X = df_seleccion.drop(columns=['RainTomorrow'])
y = df_seleccion['RainTomorrow']

# Divide los datos en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Crear un pipeline que primero preprocesa los datos y luego ajusta el modelo SVM
pipeline = Pipeline(steps=[
    ('scaler', StandardScaler()),  # Escalaa las características para mejorar el rendimiento del modelo
    ('svm', SVC(kernel='rbf', random_state=42))
])

# Ajusta el modelo SVM
pipeline.fit(X_train, y_train)

#pd: la ejecucion me tardo aprox 10 minutos, pero compila sin errores.

# Evaluar el modelo
accuracy = pipeline.score(X_test, y_test)
print("Precisión del modelo SVM:", accuracy)

#Guardar el modelo entrenado
import joblib
joblib.dump(model, 'modelo_final.pkl')
joblib.dump(scaler, 'scaler.pkl')

"""El resultado de este modelo es:

Precisión del modelo SVM: 0.831 sugiere que el modelo tiene un buen rendimiento en general.

## Evaluacion de modelos no supervisados

### KMEANS
"""

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score

# Selecciona las características importantes para la segmentación
caracteristicas = ['MinTemp', 'MaxTemp', 'Rainfall', 'Humidity9am', 'Humidity3pm']

# Crea un nuevo DataFrame con las características seleccionadas
df_seleccion = limpieza_df[caracteristicas].copy()

# Normaliza los datos para que todas las características estén en la misma escala
scaler = StandardScaler()
df_seleccion_scaled = scaler.fit_transform(df_seleccion)

# Determina el número óptimo de clústeres utilizando el método del codo
inertia = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(df_seleccion_scaled)
    inertia.append(kmeans.inertia_)

# Grafica el método del codo
plt.plot(range(1, 11), inertia, marker='o')
plt.xlabel('Número de clústeres')
plt.ylabel('Inertia')
plt.title('Método del codo para determinar el número óptimo de clústeres')
plt.show()

# Basado en el método del codo, selecciona un número óptimo de clústeres
k_optimo = 3

# Aplicar K-means con el número óptimo de clústeres
kmeans = KMeans(n_clusters=k_optimo, random_state=42)
kmeans.fit(df_seleccion_scaled)

# Añade la columna 'Cluster' al DataFrame original
limpieza_df['Cluster'] = kmeans.labels_

# Visualizar los clústeres en un diagrama de dispersión (por ejemplo, para las características MinTemp y MaxTemp)
plt.scatter(limpieza_df['MinTemp'], limpieza_df['MaxTemp'], c=limpieza_df['Cluster'], cmap='viridis')
plt.xlabel('Temperatura Mínima')
plt.ylabel('Temperatura Máxima')
plt.title('Segmentación de ubicaciones basada en características climáticas')
plt.colorbar(label='Clúster')
plt.show()

# Evaluación del modelo
# Coeficiente de Silueta
silhouette_avg = silhouette_score(df_seleccion_scaled, limpieza_df['Cluster'])

# Índice de Davies-Bouldin
db_index = davies_bouldin_score(df_seleccion_scaled, limpieza_df['Cluster'])

print(f'Coeficiente de Silueta: {silhouette_avg}')
print(f'Índice de Davies-Bouldin: {db_index}')

"""Los resultados de este modelo son:

Coeficiente de Silueta: 0.279 indica que los clústeres no están claramente separados

Índice de Davies-Bouldin: 1.270 es razonable, pero hay espacio para mejora.

### Clustering jerarquico
"""

# Función para preparar los datos
def prepare_data(df, sample_size=None):
    # Convertir las columnas 'RainToday' y 'RainTomorrow' a binarias
    if 'RainToday' in df.columns:
        df['RainToday'] = df['RainToday'].map({'No': 0, 'Yes': 1})
    if 'RainTomorrow' in df.columns:
        df['RainTomorrow'] = df['RainTomorrow'].map({'No': 0, 'Yes': 1})

    # Seleccionar características relevantes
    caracteristicas = ['MinTemp', 'MaxTemp', 'Rainfall', 'Humidity9am', 'Humidity3pm', 'Temp9am', 'Temp3pm', 'RainTomorrow']
    df_seleccion = df[caracteristicas].dropna()

    if sample_size is not None:
        df_seleccion = df_seleccion.sample(n=sample_size, random_state=42)

    # Normalizar los datos
    scaler = StandardScaler()
    df_scaled = scaler.fit_transform(df_seleccion)

    return df_seleccion, df_scaled

# Utilizar una muestra del dataset (p.ej., 5000 filas) para manejar problemas de memoria
df_seleccion, df_scaled = prepare_data(df, sample_size=5000)

# Dividir los datos en conjuntos de entrenamiento y prueba
from sklearn.model_selection import train_test_split
X_train, X_test = train_test_split(df_scaled, test_size=0.3, random_state=42)

from scipy.cluster.hierarchy import dendrogram, linkage, fcluster

# Aplicar clustering jerárquico al conjunto de entrenamiento
linked = linkage(X_train, method='ward')

# Crear el dendrograma
plt.figure(figsize=(10, 7))
dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=True)
plt.title('Dendrograma de Clustering Jerárquico (Conjunto de Entrenamiento)')
plt.xlabel('Índice de Muestra')
plt.ylabel('Distancia')
plt.show()

# Determinar el número óptimo de clústeres
num_clusters = 3  # Ajustar basado en el dendrograma

# Obtener etiquetas de clústeres para el conjunto de entrenamiento
etiquetas_train = fcluster(linked, num_clusters, criterion='maxclust')

# Aplicar clustering jerárquico al conjunto de prueba
linked_test = linkage(X_test, method='ward')
etiquetas_test = fcluster(linked_test, num_clusters, criterion='maxclust')

from sklearn.metrics import silhouette_score

# Calcular el Silhouette Score para el conjunto de entrenamiento
silhouette_train = silhouette_score(X_train, etiquetas_train)
print(f'Silhouette Score del modelo de clustering jerárquico (Conjunto de Entrenamiento): {silhouette_train}')

# Calcular el Silhouette Score para el conjunto de prueba
silhouette_test = silhouette_score(X_test, etiquetas_test)
print(f'Silhouette Score del modelo de clustering jerárquico (Conjunto de Prueba): {silhouette_test}')

# Añadir etiquetas de clúster al DataFrame original (para análisis)
df_seleccion_train = pd.DataFrame(X_train, columns=['MinTemp', 'MaxTemp', 'Rainfall', 'Humidity9am', 'Humidity3pm', 'Temp9am', 'Temp3pm', 'RainTomorrow'])
df_seleccion_train['Cluster'] = etiquetas_train

df_seleccion_test = pd.DataFrame(X_test, columns=['MinTemp', 'MaxTemp', 'Rainfall', 'Humidity9am', 'Humidity3pm', 'Temp9am', 'Temp3pm', 'RainTomorrow'])
df_seleccion_test['Cluster'] = etiquetas_test

# Analizar patrones climáticos en cada clúster (Conjunto de Entrenamiento)
for cluster in range(1, num_clusters + 1):
    print(f'\nPatrones climáticos en el clúster {cluster} (Conjunto de Entrenamiento):')
    print(df_seleccion_train[df_seleccion_train['Cluster'] == cluster].describe())

# Analizar patrones climáticos en cada clúster (Conjunto de Prueba)
for cluster in range(1, num_clusters + 1):
    print(f'\nPatrones climáticos en el clúster {cluster} (Conjunto de Prueba):')
    print(df_seleccion_test[df_seleccion_test['Cluster'] == cluster].describe())

# Visualizar clústeres en un diagrama de dispersión (por ejemplo, Temp9am y Temp3pm)
plt.scatter(df_seleccion_train['Temp9am'], df_seleccion_train['Temp3pm'], c=df_seleccion_train['Cluster'], cmap='viridis')
plt.xlabel('Temperatura a las 9am')
plt.ylabel('Temperatura a las 3pm')
plt.title('Segmentación de ubicaciones basada en características climáticas (Conjunto de Entrenamiento)')
plt.colorbar(label='Clúster')
plt.show()

plt.scatter(df_seleccion_test['Temp9am'], df_seleccion_test['Temp3pm'], c=df_seleccion_test['Cluster'], cmap='viridis')
plt.xlabel('Temperatura a las 9am')
plt.ylabel('Temperatura a las 3pm')
plt.title('Segmentación de ubicaciones basada en características climáticas (Conjunto de Prueba)')
plt.colorbar(label='Clúster')
plt.show()

"""Silhouette Score del modelo de clustering jerárquico (Conjunto de Entrenamiento): 0.263
Silhouette Score del modelo de clustering jerárquico (Conjunto de Prueba): 0.245

Los resultados indican que hay alguna separación entre los clústeres, pero no es muy fuerte. Esto sugiere que los patrones climáticos regionales en Australia que influyen en la ocurrencia de lluvias significativas no están claramente diferenciados en los datos utilizados.

## Resumen

1.   Regresión lineal múltiple y Regresión lineal: los resultados sugieren que la regresión lineal múltiple explica una mayor variabilidad en los datos en comparación con la regresión lineal simple.

2.   Random Forest: indica que el modelo es efectivo para clasificar eventos climáticos significativos basados en la humedad, con una mejoría en la precisión al considerar niveles específicos de humedad.

3.   SVM: indica una capacidad sólida para predecir la ocurrencia de lluvias en función de las características climáticas analizadas.

4.   K-Means: sugiere que el algoritmo identifica grupos de ubicaciones con características climáticas similares de manera razonable, aunque hay margen para mejorar la separación entre los clústeres.

5.   Clustering Jerárquico: indican que los clústeres formados tienen alguna estructura interna, pero podrían mejorarse para obtener una segmentación más clara de los patrones climáticos regionales.

cada modelo ofrece perspectivas diferentes sobre los datos climáticos de Australia. La regresión lineal múltiple destaca por su capacidad para explicar la variabilidad, los bosques aleatorios por su capacidad de clasificación basada en la humedad, la SVM por su alta precisión en la predicción de lluvias, y tanto K-Means como el clustering jerárquico por su capacidad para identificar patrones climáticos regionales.

Estos resultados pueden guiar decisiones relacionadas con la planificación agrícola y la gestión de recursos hídricos en diferentes regiones de Australia.

# Fase 6: Despliegue

Consideraría la Máquina de Soporte Vectorial (SVM) debido a su alta precisión en la predicción de lluvias. Esto es especialmente útil si el objetivo del panel es proporcionar alertas tempranas o recomendaciones basadas en la probabilidad de lluvia.

Sin embargo, si la interpretabilidad del modelo es crucial, la Regresión Lineal Múltiple podría ser preferible, ya que proporciona una explicación más directa de las relaciones entre las variables climáticas y la ocurrencia de lluvias.
"""

#Crear un dataframe para ver la calidad del modelo
import pandas as pd

# Definir las columnas
columnas = ["Location", "MinTemp", "MaxTemp", "Rainfall", "Evaporation", "Sunshine", "WindGustDir",
            "WindGustSpeed", "WindDir9am", "WindDir3pm", "WindSpeed9am", "WindSpeed3pm", "Humidity9am",
            "Humidity3pm", "Pressure9am", "Pressure3pm", "Cloud9am", "Cloud3pm", "Temp9am", "Temp3pm",
            "RainToday", "RISK_MM", "RainTomorrow"]

# Definir los nuevos valores
valores = [
    [1, 10.4, 24.9, 0.8, 0.0, 5.0, 2, 35.0, 17, 36, 19.0, 23.0, 61.0, 28.0, 1008.7, 1006.1, 6.0, 7.0, 15.9, 22.8, 0, 0.0, 0],
    [1, 12.7, 26.1, 0.0, 0.1, 7.0, 3, 40.0, 18, 34, 9.0, 21.0, 54.0, 20.0, 1011.5, 1009.8, 5.0, 6.0, 18.2, 25.3, 0, 0.0, 0],
    [1, 11.9, 27.5, 0.0, 0.0, 9.0, 4, 48.0, 16, 38, 18.0, 25.0, 50.0, 29.0, 1009.6, 1010.7, 7.0, 3.0, 20.0, 24.2, 0, 0.0, 0],
    [1, 8.3, 29.0, 0.0, 0.0, 4.0, 1, 26.0, 19, 35, 10.0, 8.0, 48.0, 19.0, 1018.6, 1013.8, 8.0, 7.0, 16.1, 27.5, 0, 1.5, 1],
    [1, 16.5, 33.3, 1.2, 0.2, 3.0, 2, 42.0, 20, 37, 8.0, 18.0, 79.0, 31.0, 1011.8, 1007.0, 6.0, 2.0, 17.6, 28.7, 0, 0.1, 0]
]

# Crear el DataFrame
df = pd.DataFrame(valores, columns=columnas)
print(df)

X_prueba = df.drop(['RainTomorrow'], axis=1)
y_prueba = df['RainTomorrow']

# Cargar el escalador guardado
scaler = joblib.load('scaler.pkl')

# Estandarizar los datos de prueba con el escalador cargado
X_prueba_scaled = scaler.transform(X_prueba)

# Cargar el modelo guardado
loaded_model = joblib.load('modelo_final.pkl')

# Hacer predicciones con el modelo cargado
y_pred_prueba = loaded_model.predict(X_prueba_scaled)

# Evaluar el modelo
accuracy = accuracy_score(y_prueba, y_pred_prueba)
print(f"Exactitud del modelo cargado: {accuracy}")

# Mostrar predicciones y valores reales
resultados = pd.DataFrame({'Predecida': y_pred_prueba, 'Actual': y_prueba})
print(resultados)

"""El modelo SVM en este ejemplo está diseñado para predecir si lloverá mañana (RainTomorrow). La variable objetivo RainTomorrow tiene valores binarios: 0 (No) y 1 (Sí). Por lo tanto, el modelo debería predecir una de estas dos clases basado en las características meteorológicas ingresadas por el usuario.

El modelo SVM ha demostrado un nivel de predicción del 80%. En cinco intentos, ha cometido dos errores.

**Insights**

Las variables como la temperatura mínima y máxima, la cantidad de lluvia caída, y los niveles de humedad a las 9 am y 3 pm han mostrado correlación en la predicción de lluvia. Estos factores pueden ser monitoreados de cerca para mejorar la precisión del modelo.

Con un modelo predictivo que tiene una precisión del 80%, las operaciones que dependen del clima pueden ser planificadas. Esto es útil en sectores como la agricultura, la construcción, la planificación de eventos al aire libre y turismo.

La capacidad de predecir la lluvia con precisión puede ayudar a las organizaciones a gestionar mejor los recursos, como la preparación de infraestructura para lluvias intensas o la optimización del riego en la agricultura.

**Propuestas**

Optimización del Riego en Agricultura

Utilizar las predicciones del modelo para optimizar los sistemas de riego en las operaciones agrícolas.

Mejora Continua del Modelo

Implementar un ciclo de mejora continua donde se recopilen más datos y se entrene el modelo periódicamente.

Políticas de Gestión de Emergencias frente a sequías o inundaciones

Desarrollar o actualizar políticas de gestión de emergencias y protocolos de respuesta basados en los datos predictivos del modelo frente a periodos prolongados de altas precipitaciones o sequías.
Conclusión sobre la evaluación
"""